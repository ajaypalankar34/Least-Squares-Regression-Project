#!/usr/bin/env python
# coding: utf-8

# In[122]:



import numpy as np 
import math


# In[123]:


# Problem 2.1 1st method (fixed step size)
def gradient_descent(x,y): 
    a_curr = 0 
    b_curr = 0
    iterations = 100
    M = 100
    learning_rate_gamma = 0.05

    for m in range(iterations):
        y_predicted = ((a_curr*x) + b_curr)
        cost = (1/M)*sum([val ** 2 for val in (y - y_predicted)])
        ad = -(2/M) *sum(x*(y-y_predicted))
        bd = -(2/M) *sum(y-y_predicted)
        a_curr = a_curr - learning_rate_gamma * ad
        b_curr = b_curr - learning_rate_gamma * bd
        print ("a {}, b {}, cost{} iteration()".format(a_curr,b_curr, cost, m))
        
x = np.array([-0.204766148,
-2.721570643,
4.008524885,
3.451781851,
0.859870358,
1.664162173,
1.259597852,
2.297518553,
4.823032229,
0.814464879,
0.800903658,
-3.791404289,
-0.157034888,
-2.90594916,
1.298833851,
1.147134191,
-4.50467421,
-3.074896039,
-2.945058291,
-3.109278255,
1.351979169,
0.38596678,
-0.008839865,
-0.548168347,
-0.096427065,
3.739274059,
-2.915386412,
1.403118252,
-2.940244845,
-4.179287929,
-3.579588781,
1.209586439,
-4.479221097,
2.286616817,
-4.365954993,
4.34405119,
3.589388167,
0.133774186,
-1.014105033,
-4.691104513,
-1.986939354,
-1.670637182,
1.481984065,
3.422066124,
3.540999493,
-0.539733519,
-3.228924662,
-1.691710048,
-3.818448016,
0.39982099,
4.994916201,
-0.854774611,
2.639570785,
-3.997784598,
-1.403650865,
0.218856737,
-3.243309703,
4.05153559,
-0.315318001,
-3.959884252,
2.362674556,
-3.158059025,
-2.000630099,
-2.873984666,
-4.285471872,
-4.462456078,
-4.867167995,
-3.033418086,
-1.926331004,
-3.983306064,
-1.679071665,
-4.379547787,
-4.536487351,
2.614258867,
-4.101083492,
2.772405365,
0.337719518,
3.258088579,
-2.06026947,
-4.896633817,
1.679161216,
0.261024658,
2.072534853,
-2.120230244,
0.56669835,
-4.384093329,
-1.624161359,
-3.95186758,
0.49540107,
3.904756792,
2.343410837,
-4.271147009,
2.983508641,
1.837155724,
2.227245397,
-3.825071478,
-1.711857852,
2.491314631,
2.40032328,
2.349575417
])
y = np.array([-1.211629135,
12.17042185,
8.325192121,
8.423588631,
-2.975786002,
-1.772755799,
1.395671161,
4.651909988,
12.31696158,
-0.273282351,
-4.226736034,
-12.92483722,
-3.306258109,
-12.58881399,
-2.826528358,
-0.922283863,
-17.58908901,
-17.48064165,
-14.81623931,
-16.77869034,
-3.132429935,
-1.267966306,
-3.840302075,
-8.609857263,
-2.367630773,
6.357409691,
-12.41346425,
-0.244546175,
-8.772069335,
-17.55780788,
-16.01102284,
-0.095144733,
-15.88365624,
4.154612219,
-15.00070346,
14.04969852,
9.082007807,
-6.175449448,
-8.557898931,
-16.96303642,
-11.75104433,
-9.333685078,
-1.4593198,
-6.422238535,
-6.196657505,
-7.961912194,
-13.06575735,
-8.067642032,
-11.63831939,
-0.706470495,
-10.42397283,
-6.889327716,
-5.030225898,
-19.05873982,
-11.04249924,
-4.165931641,
-16.45185537,
-9.033428993,
-2.903593767,
-15.05025217,
-3.786532501,
-12.82049658,
-11.7947826,
-10.54632272,
-17.2022433,
-17.98162183,
-15.52480988,
-15.95312658,
-10.20552739,
-12.06112261,
-9.537279552,
-18.09340869,
-17.54886401,
-4.651283531,
-19.56433626,
-8.417301562,
-4.966644655,
-4.588953299,
-8.408053655,
-22.60340936,
-1.838959644,
-2.223557243,
-4.158500136,
-8.740746585,
-3.310990088,
-15.85833811,
-8.779615025,
-18.95663169,
-2.638075039,
-9.317678516,
-1.532479006,
-19.35161457,
-10.52868818,
-0.034661032,
-0.42507557,
-14.0403298,
-8.503601796,
-4.276193165,
-0.01063457,
-7.325730759
])


# In[56]:


gradient_descent(x,y)


# In[107]:


#Problem 2.1 second method (diminishing step size, p/q+t)
def gradient_descent(x,y): 
    a_curr = 0 
    b_curr = 0
    p = 1
    q = 1
    t = 29
    iterations = 100
    M = len(x)
    learning_rate_diminishing = (p)/(q+t)

    for m in range(iterations):
        y_predicted = ((a_curr*x) + b_curr)
        cost = (1/M)*sum([val ** 2 for val in (y - y_predicted)])
        ad = (-2/M) *sum(x*(y-y_predicted))
        bd = (-2/M) *sum(y-y_predicted)
        a_curr = a_curr - learning_rate_diminishing * ad
        b_curr = b_curr - learning_rate_diminishing * bd
        print ("a {}, b {}, cost{} iteration()".format(a_curr,b_curr, cost, m))

x = np.array([-0.204766148,
-2.721570643,
4.008524885,
3.451781851,
0.859870358,
1.664162173,
1.259597852,
2.297518553,
4.823032229,
0.814464879,
0.800903658,
-3.791404289,
-0.157034888,
-2.90594916,
1.298833851,
1.147134191,
-4.50467421,
-3.074896039,
-2.945058291,
-3.109278255,
1.351979169,
0.38596678,
-0.008839865,
-0.548168347,
-0.096427065,
3.739274059,
-2.915386412,
1.403118252,
-2.940244845,
-4.179287929,
-3.579588781,
1.209586439,
-4.479221097,
2.286616817,
-4.365954993,
4.34405119,
3.589388167,
0.133774186,
-1.014105033,
-4.691104513,
-1.986939354,
-1.670637182,
1.481984065,
3.422066124,
3.540999493,
-0.539733519,
-3.228924662,
-1.691710048,
-3.818448016,
0.39982099,
4.994916201,
-0.854774611,
2.639570785,
-3.997784598,
-1.403650865,
0.218856737,
-3.243309703,
4.05153559,
-0.315318001,
-3.959884252,
2.362674556,
-3.158059025,
-2.000630099,
-2.873984666,
-4.285471872,
-4.462456078,
-4.867167995,
-3.033418086,
-1.926331004,
-3.983306064,
-1.679071665,
-4.379547787,
-4.536487351,
2.614258867,
-4.101083492,
2.772405365,
0.337719518,
3.258088579,
-2.06026947,
-4.896633817,
1.679161216,
0.261024658,
2.072534853,
-2.120230244,
0.56669835,
-4.384093329,
-1.624161359,
-3.95186758,
0.49540107,
3.904756792,
2.343410837,
-4.271147009,
2.983508641,
1.837155724,
2.227245397,
-3.825071478,
-1.711857852,
2.491314631,
2.40032328,
2.349575417
])
y = np.array([-1.211629135,
12.17042185,
8.325192121,
8.423588631,
-2.975786002,
-1.772755799,
1.395671161,
4.651909988,
12.31696158,
-0.273282351,
-4.226736034,
-12.92483722,
-3.306258109,
-12.58881399,
-2.826528358,
-0.922283863,
-17.58908901,
-17.48064165,
-14.81623931,
-16.77869034,
-3.132429935,
-1.267966306,
-3.840302075,
-8.609857263,
-2.367630773,
6.357409691,
-12.41346425,
-0.244546175,
-8.772069335,
-17.55780788,
-16.01102284,
-0.095144733,
-15.88365624,
4.154612219,
-15.00070346,
14.04969852,
9.082007807,
-6.175449448,
-8.557898931,
-16.96303642,
-11.75104433,
-9.333685078,
-1.4593198,
-6.422238535,
-6.196657505,
-7.961912194,
-13.06575735,
-8.067642032,
-11.63831939,
-0.706470495,
-10.42397283,
-6.889327716,
-5.030225898,
-19.05873982,
-11.04249924,
-4.165931641,
-16.45185537,
-9.033428993,
-2.903593767,
-15.05025217,
-3.786532501,
-12.82049658,
-11.7947826,
-10.54632272,
-17.2022433,
-17.98162183,
-15.52480988,
-15.95312658,
-10.20552739,
-12.06112261,
-9.537279552,
-18.09340869,
-17.54886401,
-4.651283531,
-19.56433626,
-8.417301562,
-4.966644655,
-4.588953299,
-8.408053655,
-22.60340936,
-1.838959644,
-2.223557243,
-4.158500136,
-8.740746585,
-3.310990088,
-15.85833811,
-8.779615025,
-18.95663169,
-2.638075039,
-9.317678516,
-1.532479006,
-19.35161457,
-10.52868818,
-0.034661032,
-0.42507557,
-14.0403298,
-8.503601796,
-4.276193165,
-0.01063457,
-7.325730759
])


# In[108]:


gradient_descent(x,y)


# In[109]:


#Problem 2.1 third method (exact line search)
def gradient_descent(x,y): 
    a_curr = 0 
    b_curr = 0
    iterations = 100
    M = len(x)
    learning_rate_exact_line_alpha = 0.03

    for m in range(iterations):
        y_predicted = ((a_curr*x) + b_curr)
        cost = (1/M)*sum([val ** 2 for val in (y - y_predicted)])
        ad = (-2/M) *sum(x*(y-y_predicted))
        bd = (-2/M) *sum(y-y_predicted)
        a_curr = a_curr - learning_rate_exact_line_alpha * ad
        b_curr = b_curr - learning_rate_exact_line_alpha * bd
        print ("a {}, b {}, cost{} iteration()".format(a_curr,b_curr, cost, m))
        

x = np.array([-0.204766148,
-2.721570643,
4.008524885,
3.451781851,
0.859870358,
1.664162173,
1.259597852,
2.297518553,
4.823032229,
0.814464879,
0.800903658,
-3.791404289,
-0.157034888,
-2.90594916,
1.298833851,
1.147134191,
-4.50467421,
-3.074896039,
-2.945058291,
-3.109278255,
1.351979169,
0.38596678,
-0.008839865,
-0.548168347,
-0.096427065,
3.739274059,
-2.915386412,
1.403118252,
-2.940244845,
-4.179287929,
-3.579588781,
1.209586439,
-4.479221097,
2.286616817,
-4.365954993,
4.34405119,
3.589388167,
0.133774186,
-1.014105033,
-4.691104513,
-1.986939354,
-1.670637182,
1.481984065,
3.422066124,
3.540999493,
-0.539733519,
-3.228924662,
-1.691710048,
-3.818448016,
0.39982099,
4.994916201,
-0.854774611,
2.639570785,
-3.997784598,
-1.403650865,
0.218856737,
-3.243309703,
4.05153559,
-0.315318001,
-3.959884252,
2.362674556,
-3.158059025,
-2.000630099,
-2.873984666,
-4.285471872,
-4.462456078,
-4.867167995,
-3.033418086,
-1.926331004,
-3.983306064,
-1.679071665,
-4.379547787,
-4.536487351,
2.614258867,
-4.101083492,
2.772405365,
0.337719518,
3.258088579,
-2.06026947,
-4.896633817,
1.679161216,
0.261024658,
2.072534853,
-2.120230244,
0.56669835,
-4.384093329,
-1.624161359,
-3.95186758,
0.49540107,
3.904756792,
2.343410837,
-4.271147009,
2.983508641,
1.837155724,
2.227245397,
-3.825071478,
-1.711857852,
2.491314631,
2.40032328,
2.349575417
])
y = np.array([-1.211629135,
12.17042185,
8.325192121,
8.423588631,
-2.975786002,
-1.772755799,
1.395671161,
4.651909988,
12.31696158,
-0.273282351,
-4.226736034,
-12.92483722,
-3.306258109,
-12.58881399,
-2.826528358,
-0.922283863,
-17.58908901,
-17.48064165,
-14.81623931,
-16.77869034,
-3.132429935,
-1.267966306,
-3.840302075,
-8.609857263,
-2.367630773,
6.357409691,
-12.41346425,
-0.244546175,
-8.772069335,
-17.55780788,
-16.01102284,
-0.095144733,
-15.88365624,
4.154612219,
-15.00070346,
14.04969852,
9.082007807,
-6.175449448,
-8.557898931,
-16.96303642,
-11.75104433,
-9.333685078,
-1.4593198,
-6.422238535,
-6.196657505,
-7.961912194,
-13.06575735,
-8.067642032,
-11.63831939,
-0.706470495,
-10.42397283,
-6.889327716,
-5.030225898,
-19.05873982,
-11.04249924,
-4.165931641,
-16.45185537,
-9.033428993,
-2.903593767,
-15.05025217,
-3.786532501,
-12.82049658,
-11.7947826,
-10.54632272,
-17.2022433,
-17.98162183,
-15.52480988,
-15.95312658,
-10.20552739,
-12.06112261,
-9.537279552,
-18.09340869,
-17.54886401,
-4.651283531,
-19.56433626,
-8.417301562,
-4.966644655,
-4.588953299,
-8.408053655,
-22.60340936,
-1.838959644,
-2.223557243,
-4.158500136,
-8.740746585,
-3.310990088,
-15.85833811,
-8.779615025,
-18.95663169,
-2.638075039,
-9.317678516,
-1.532479006,
-19.35161457,
-10.52868818,
-0.034661032,
-0.42507557,
-14.0403298,
-8.503601796,
-4.276193165,
-0.01063457,
-7.325730759
])


# In[110]:


gradient_descent(x,y)


# In[111]:


#Problem 2.1 4th method (Backtracking line search)
def gradient_descent(x,y): 
    a_curr = 0 
    b_curr = 0
    iterations = 100
    M = len(x)
    alpha = 0.03
    beta = 0.01
    learning_rate_backtrack_line = alpha - beta

    for m in range(iterations):
        y_predicted = ((a_curr*x) + b_curr)
        cost = (1/M)*sum([val ** 2 for val in (y - y_predicted)])
        ad = (-2/M) *sum(x*(y-y_predicted))
        bd = (-2/M) *sum(y-y_predicted)
        a_curr = a_curr - learning_rate_backtrack_line * ad
        b_curr = b_curr - learning_rate_backtrack_line * bd
        print ("a {}, b {}, cost{} iteration()".format(a_curr,b_curr, cost, m))

        x = np.array([-0.204766148,
-2.721570643,
4.008524885,
3.451781851,
0.859870358,
1.664162173,
1.259597852,
2.297518553,
4.823032229,
0.814464879,
0.800903658,
-3.791404289,
-0.157034888,
-2.90594916,
1.298833851,
1.147134191,
-4.50467421,
-3.074896039,
-2.945058291,
-3.109278255,
1.351979169,
0.38596678,
-0.008839865,
-0.548168347,
-0.096427065,
3.739274059,
-2.915386412,
1.403118252,
-2.940244845,
-4.179287929,
-3.579588781,
1.209586439,
-4.479221097,
2.286616817,
-4.365954993,
4.34405119,
3.589388167,
0.133774186,
-1.014105033,
-4.691104513,
-1.986939354,
-1.670637182,
1.481984065,
3.422066124,
3.540999493,
-0.539733519,
-3.228924662,
-1.691710048,
-3.818448016,
0.39982099,
4.994916201,
-0.854774611,
2.639570785,
-3.997784598,
-1.403650865,
0.218856737,
-3.243309703,
4.05153559,
-0.315318001,
-3.959884252,
2.362674556,
-3.158059025,
-2.000630099,
-2.873984666,
-4.285471872,
-4.462456078,
-4.867167995,
-3.033418086,
-1.926331004,
-3.983306064,
-1.679071665,
-4.379547787,
-4.536487351,
2.614258867,
-4.101083492,
2.772405365,
0.337719518,
3.258088579,
-2.06026947,
-4.896633817,
1.679161216,
0.261024658,
2.072534853,
-2.120230244,
0.56669835,
-4.384093329,
-1.624161359,
-3.95186758,
0.49540107,
3.904756792,
2.343410837,
-4.271147009,
2.983508641,
1.837155724,
2.227245397,
-3.825071478,
-1.711857852,
2.491314631,
2.40032328,
2.349575417
])
y = np.array([-1.211629135,
12.17042185,
8.325192121,
8.423588631,
-2.975786002,
-1.772755799,
1.395671161,
4.651909988,
12.31696158,
-0.273282351,
-4.226736034,
-12.92483722,
-3.306258109,
-12.58881399,
-2.826528358,
-0.922283863,
-17.58908901,
-17.48064165,
-14.81623931,
-16.77869034,
-3.132429935,
-1.267966306,
-3.840302075,
-8.609857263,
-2.367630773,
6.357409691,
-12.41346425,
-0.244546175,
-8.772069335,
-17.55780788,
-16.01102284,
-0.095144733,
-15.88365624,
4.154612219,
-15.00070346,
14.04969852,
9.082007807,
-6.175449448,
-8.557898931,
-16.96303642,
-11.75104433,
-9.333685078,
-1.4593198,
-6.422238535,
-6.196657505,
-7.961912194,
-13.06575735,
-8.067642032,
-11.63831939,
-0.706470495,
-10.42397283,
-6.889327716,
-5.030225898,
-19.05873982,
-11.04249924,
-4.165931641,
-16.45185537,
-9.033428993,
-2.903593767,
-15.05025217,
-3.786532501,
-12.82049658,
-11.7947826,
-10.54632272,
-17.2022433,
-17.98162183,
-15.52480988,
-15.95312658,
-10.20552739,
-12.06112261,
-9.537279552,
-18.09340869,
-17.54886401,
-4.651283531,
-19.56433626,
-8.417301562,
-4.966644655,
-4.588953299,
-8.408053655,
-22.60340936,
-1.838959644,
-2.223557243,
-4.158500136,
-8.740746585,
-3.310990088,
-15.85833811,
-8.779615025,
-18.95663169,
-2.638075039,
-9.317678516,
-1.532479006,
-19.35161457,
-10.52868818,
-0.034661032,
-0.42507557,
-14.0403298,
-8.503601796,
-4.276193165,
-0.01063457,
-7.325730759
])


# In[112]:


gradient_descent(x,y)


# In[113]:


#Problem 2.2 first method (diminishing step size, p/q+t, for a given p > 0 and q)
def gradient_descent(x,y): 
    a_curr = 0 
    b_curr = 0
    iterations = 100
    M = len(x)
    p = 1
    q = 1
    t = 29
    learning_rate_diminishing = (p)/(q+t)

    for m in range(iterations):
        y_predicted = ((a_curr*x) + b_curr)
        cost = (1/M)*sum([abs(val) for val in (y - y_predicted)])
        ad = (-2/M) *sum(x*(y-y_predicted))
        bd = (-2/M) *sum(y-y_predicted)
        a_curr = a_curr - learning_rate_diminishing * ad
        b_curr = b_curr - learning_rate_diminishing * bd
        print ("a {}, b {}, cost{} iteration()".format(a_curr,b_curr, cost, m))
        
        x = np.array([-0.204766148,
-2.721570643,
4.008524885,
3.451781851,
0.859870358,
1.664162173,
1.259597852,
2.297518553,
4.823032229,
0.814464879,
0.800903658,
-3.791404289,
-0.157034888,
-2.90594916,
1.298833851,
1.147134191,
-4.50467421,
-3.074896039,
-2.945058291,
-3.109278255,
1.351979169,
0.38596678,
-0.008839865,
-0.548168347,
-0.096427065,
3.739274059,
-2.915386412,
1.403118252,
-2.940244845,
-4.179287929,
-3.579588781,
1.209586439,
-4.479221097,
2.286616817,
-4.365954993,
4.34405119,
3.589388167,
0.133774186,
-1.014105033,
-4.691104513,
-1.986939354,
-1.670637182,
1.481984065,
3.422066124,
3.540999493,
-0.539733519,
-3.228924662,
-1.691710048,
-3.818448016,
0.39982099,
4.994916201,
-0.854774611,
2.639570785,
-3.997784598,
-1.403650865,
0.218856737,
-3.243309703,
4.05153559,
-0.315318001,
-3.959884252,
2.362674556,
-3.158059025,
-2.000630099,
-2.873984666,
-4.285471872,
-4.462456078,
-4.867167995,
-3.033418086,
-1.926331004,
-3.983306064,
-1.679071665,
-4.379547787,
-4.536487351,
2.614258867,
-4.101083492,
2.772405365,
0.337719518,
3.258088579,
-2.06026947,
-4.896633817,
1.679161216,
0.261024658,
2.072534853,
-2.120230244,
0.56669835,
-4.384093329,
-1.624161359,
-3.95186758,
0.49540107,
3.904756792,
2.343410837,
-4.271147009,
2.983508641,
1.837155724,
2.227245397,
-3.825071478,
-1.711857852,
2.491314631,
2.40032328,
2.349575417
])
y = np.array([-1.211629135,
12.17042185,
8.325192121,
8.423588631,
-2.975786002,
-1.772755799,
1.395671161,
4.651909988,
12.31696158,
-0.273282351,
-4.226736034,
-12.92483722,
-3.306258109,
-12.58881399,
-2.826528358,
-0.922283863,
-17.58908901,
-17.48064165,
-14.81623931,
-16.77869034,
-3.132429935,
-1.267966306,
-3.840302075,
-8.609857263,
-2.367630773,
6.357409691,
-12.41346425,
-0.244546175,
-8.772069335,
-17.55780788,
-16.01102284,
-0.095144733,
-15.88365624,
4.154612219,
-15.00070346,
14.04969852,
9.082007807,
-6.175449448,
-8.557898931,
-16.96303642,
-11.75104433,
-9.333685078,
-1.4593198,
-6.422238535,
-6.196657505,
-7.961912194,
-13.06575735,
-8.067642032,
-11.63831939,
-0.706470495,
-10.42397283,
-6.889327716,
-5.030225898,
-19.05873982,
-11.04249924,
-4.165931641,
-16.45185537,
-9.033428993,
-2.903593767,
-15.05025217,
-3.786532501,
-12.82049658,
-11.7947826,
-10.54632272,
-17.2022433,
-17.98162183,
-15.52480988,
-15.95312658,
-10.20552739,
-12.06112261,
-9.537279552,
-18.09340869,
-17.54886401,
-4.651283531,
-19.56433626,
-8.417301562,
-4.966644655,
-4.588953299,
-8.408053655,
-22.60340936,
-1.838959644,
-2.223557243,
-4.158500136,
-8.740746585,
-3.310990088,
-15.85833811,
-8.779615025,
-18.95663169,
-2.638075039,
-9.317678516,
-1.532479006,
-19.35161457,
-10.52868818,
-0.034661032,
-0.42507557,
-14.0403298,
-8.503601796,
-4.276193165,
-0.01063457,
-7.325730759
])


# In[114]:


gradient_descent(x,y)


# In[115]:


#Problem 2.2, second method (diminishing step size, p/sqrt(t))
def gradient_descent(x,y): 
    a_curr = 0 
    b_curr = 0
    iterations = 100
    M = len(x)
    p = 1
    t = 900
    learning_rate_diminishing = (p)/(math.sqrt(t))

    for m in range(iterations):
        y_predicted = ((a_curr*x) + b_curr)
        cost = (1/M)*sum([abs(val) for val in (y - y_predicted)])
        ad = (-2/M) *sum(x*(y-y_predicted))
        bd = (-2/M) *sum(y-y_predicted)
        a_curr = a_curr - learning_rate_diminishing * ad
        b_curr = b_curr - learning_rate_diminishing * bd
        print ("a {}, b {}, cost{} iteration()".format(a_curr,b_curr, cost, m))

        x = np.array([-0.204766148,
-2.721570643,
4.008524885,
3.451781851,
0.859870358,
1.664162173,
1.259597852,
2.297518553,
4.823032229,
0.814464879,
0.800903658,
-3.791404289,
-0.157034888,
-2.90594916,
1.298833851,
1.147134191,
-4.50467421,
-3.074896039,
-2.945058291,
-3.109278255,
1.351979169,
0.38596678,
-0.008839865,
-0.548168347,
-0.096427065,
3.739274059,
-2.915386412,
1.403118252,
-2.940244845,
-4.179287929,
-3.579588781,
1.209586439,
-4.479221097,
2.286616817,
-4.365954993,
4.34405119,
3.589388167,
0.133774186,
-1.014105033,
-4.691104513,
-1.986939354,
-1.670637182,
1.481984065,
3.422066124,
3.540999493,
-0.539733519,
-3.228924662,
-1.691710048,
-3.818448016,
0.39982099,
4.994916201,
-0.854774611,
2.639570785,
-3.997784598,
-1.403650865,
0.218856737,
-3.243309703,
4.05153559,
-0.315318001,
-3.959884252,
2.362674556,
-3.158059025,
-2.000630099,
-2.873984666,
-4.285471872,
-4.462456078,
-4.867167995,
-3.033418086,
-1.926331004,
-3.983306064,
-1.679071665,
-4.379547787,
-4.536487351,
2.614258867,
-4.101083492,
2.772405365,
0.337719518,
3.258088579,
-2.06026947,
-4.896633817,
1.679161216,
0.261024658,
2.072534853,
-2.120230244,
0.56669835,
-4.384093329,
-1.624161359,
-3.95186758,
0.49540107,
3.904756792,
2.343410837,
-4.271147009,
2.983508641,
1.837155724,
2.227245397,
-3.825071478,
-1.711857852,
2.491314631,
2.40032328,
2.349575417
])
y = np.array([-1.211629135,
12.17042185,
8.325192121,
8.423588631,
-2.975786002,
-1.772755799,
1.395671161,
4.651909988,
12.31696158,
-0.273282351,
-4.226736034,
-12.92483722,
-3.306258109,
-12.58881399,
-2.826528358,
-0.922283863,
-17.58908901,
-17.48064165,
-14.81623931,
-16.77869034,
-3.132429935,
-1.267966306,
-3.840302075,
-8.609857263,
-2.367630773,
6.357409691,
-12.41346425,
-0.244546175,
-8.772069335,
-17.55780788,
-16.01102284,
-0.095144733,
-15.88365624,
4.154612219,
-15.00070346,
14.04969852,
9.082007807,
-6.175449448,
-8.557898931,
-16.96303642,
-11.75104433,
-9.333685078,
-1.4593198,
-6.422238535,
-6.196657505,
-7.961912194,
-13.06575735,
-8.067642032,
-11.63831939,
-0.706470495,
-10.42397283,
-6.889327716,
-5.030225898,
-19.05873982,
-11.04249924,
-4.165931641,
-16.45185537,
-9.033428993,
-2.903593767,
-15.05025217,
-3.786532501,
-12.82049658,
-11.7947826,
-10.54632272,
-17.2022433,
-17.98162183,
-15.52480988,
-15.95312658,
-10.20552739,
-12.06112261,
-9.537279552,
-18.09340869,
-17.54886401,
-4.651283531,
-19.56433626,
-8.417301562,
-4.966644655,
-4.588953299,
-8.408053655,
-22.60340936,
-1.838959644,
-2.223557243,
-4.158500136,
-8.740746585,
-3.310990088,
-15.85833811,
-8.779615025,
-18.95663169,
-2.638075039,
-9.317678516,
-1.532479006,
-19.35161457,
-10.52868818,
-0.034661032,
-0.42507557,
-14.0403298,
-8.503601796,
-4.276193165,
-0.01063457,
-7.325730759
])


# In[116]:


gradient_descent(x,y)


# In[120]:


# Problem 3.1 (letters b and c)
def gradient_descent(a): 
    
    learning_rate_fixed = 0.05
    a = 0
    first_log = np.log(exp(a)/1+exp(a))
    second_log = np.log(1/1+exp(a))
    
    cost = (exp(a)/(1+exp(a)))(first_log)+(1/1+exp(a))(second_log)
    
    ad = (aexp(a)/exp(a)+1)
    a = a - learning_rate_diminishing * ad
    print ("a {}, cost{}".format(a, cost))
    
    theta = 1
    epsilon = 0.01
    
    first_ln_part = np.log(exp(1.01)/(1+exp(1.01)))
    second_ln_part = np.log(1/1+exp(1.01))
    third_ln_part = np.log(exp(0.99)/(1+exp(0.99)))
    fourth_ln_part = np.log(1/1+exp(0.99))
    
    numerical_approximations = (exp(1.01)/(1+exp(1.01)))(first_ln_part) + (1/1+exp(1.01))(second_ln_part) - (exp(0.99)/(1+exp(0.99)))
    (third_ln_part) + (1/1+exp(0.99))(fourth_ln_part) 
    
    

